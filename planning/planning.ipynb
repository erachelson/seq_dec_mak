{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning course notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Planning is the art of producing a sequence of actions that brings an agent from given initial and goal states, knowing the model of the agent's state transition model.\n",
    "While the dynamics of the environment can be deterministic or probabilistic, the main differences with Reinforcement Learning are the following:\n",
    "- knowledge of a white-box state transition model defined as a state-to-state mapping or as a probability distribution over next states given the current state ;\n",
    "- knowledge of the reward or cost function, or more specifically of a set of goal states to reach ;\n",
    "- optimization of the sequence of actions, named a _plan_, offline without interacting with the environment but reasoning about the white-box state transition function.\n",
    "\n",
    "It is worth mentioning that the reward or cost function can result from a simulation (e.g. aircraft performance model to compute fuel computation in flight planning).\n",
    "Only the next state - or the probability distribution over next states - must be known beforehand by most planning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of a planning problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In its simplest form, a (deterministic) planning process is defined by a tuple $(S, A, T, C, s_i, G)$ where:\n",
    "- $S$ is a finite set of states ;\n",
    "- $A$ is a finite set of actions ;\n",
    "- $T : S, A \\rightarrow S$ is a transition function that maps a given state-action pair to a next state ;\n",
    "- $C : S, A \\rightarrow \\mathbb{R}$ is a cost function that assigns a cost to a given state-action pair (transition) ;\n",
    "- $s_i \\in S$ is an initial state.\n",
    "- $G \\subset S$ is a set of goal states.\n",
    "\n",
    "A _plan_ $\\pi$ is a sequence of actions: $\\pi \\in \\bigcup_{n \\in \\mathbb{N}_+} A^n$.\n",
    "The execution of any given plan $\\pi$ from the initial state $s_i$ produces a sequence of states $\\sigma(\\pi) = T( \\cdots T( T(s_i, \\pi[0]), \\pi[1]), \\cdots , \\pi[n-1]) \\in \\bigcup_{n \\in \\mathbb{N}_+} S^n$ where $\\vert \\pi \\vert$ is the length of $\\pi$.\n",
    "The cost of a plan $\\pi$ is defined by $C(\\pi) = \\sum_{0 \\leqslant n \\leqslant \\vert \\pi \\vert} c(\\sigma(\\pi)[n], \\pi[n])$, i.e. the cumulative cost obtained by executing $\\pi$ from the initial state $s_i$.\n",
    "\n",
    "A planning problem $(\\mathcal{P})$ consists in finding a plan that reaches a goal state in $G$ from the initial state $s_i$ in minimum cost:\n",
    "\n",
    "$ \\displaystyle (\\mathcal{P}) \\; \\min_{\\pi \\in \\bigcup_{n \\in \\mathbb{N}_+} A^n} C(\\pi) \\; \\text{such that} \\; \\sigma(\\pi)[n-1] \\in G$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example: the famous maze problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a maze as defined with the following code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hp/p5lfqcrx34gcc58lq7_1kvnm0000gn/T/ipykernel_53344/724660161.py:105: MatplotlibDeprecationWarning: \n",
      "The set_window_title function was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use manager.set_window_title or GUI-specific methods instead.\n",
      "  fig.canvas.set_window_title(\"Maze\")\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADrCAYAAACICmHVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAErUlEQVR4nO3dQYocVRzA4SoVDQREFDci5ArugutZuPBknsNTuJi1ZJcrCMGNGEQIqCDlAeyB6sy8fv2r/r5tF5PXVfPjDeTPq3XbtgW4fh/MXgCwj1ghQqwQIVaIECtEiBUiPjrn4o/XT7Zny/NRa+GEf7/Yf78//P3dwJVwCX8t75Z/tr/XU5+dFeuz5fnycr17mlWxyx/ff7v72s9+/HngSriEV9v9g5/5MxgixAoRYoUIsUKEWCFCrBAhVog46/9Z9/rp19cjfuxu3331za7rZq9zn9f7L/1h2CJuzojfob0/8yF2VogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRAwZNzxHYTTwsWNit2Dv85l5LxvjpQ+zs0KEWCFCrBAhVogQK0SIFSLEChFihQixQsT0CaaCEZMvhcmtZWlMHI1Y4+z7foqdFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEZlxwxEjZYUxtcIaj+gaD8mzs0KEWCFCrBAhVogQK0SIFSLEChFihQixQkRmgulor3yceQjbTJXpqWt8PnZWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIyEwwnaPwOsXCGkcoTFkty3Wu084KEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoWI6eOGhXG6whqXpbPOvWZ+HwemAe9NrBAhVogQK0SIFSLEChFihQixQoRYIWLIBNM1HjZ1SmGdhTUuy9x1zvy3LzllZWeFCLFChFghQqwQIVaIECtEiBUixAoRYoWIIRNMRzsLiKczYtroGs9LGsHOChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFiOmvfLzGsa6jO2c8b+/zuZWRv5nsrBAhVogQK0SIFSLEChFihQixQoRYIUKsEDF9gmnm5IuD3S7PPX9/dlaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0RMHzcsjAbOPLhrxOFmMxXWeK3srBAhVogQK0SIFSLEChFihQixQoRYIUKsEDF9gqmg8jrDox1GdrTv89hnbmeFCLFChFghQqwQIVaIECtEiBUixAoRYoWIzATTiImfo00RVc6Kemq38r3trBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiMiMGx7t8KyZjvYaycqBdo9lZ4UIsUKEWCFCrBAhVogQK0SIFSLEChFihYghE0yzpz9u5QCto3vq51h/NnZWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIGDLBVJkUqaxzr8L3uZXzkkaws0KEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIWL6Kx8Lo2Iz11i4P8syZp2F8clLsrNChFghQqwQIVaIECtEiBUixAoRYoUIsULE9AmmwgFatzxJs/dezrxHt/J87KwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVohYt23bffGn6+fby/Vu4HK4iPuv919792bcOvifV9v98uf2dj31mZ0VIsQKEWKFCLFChFghQqwQIVaIECtEiBUizppgWtf1t2VZfhm3HLh5L7Zt+/LUB2fFCszjz2CIECtEiBUixAoRYoUIsUKEWCFCrBAhVoj4DwE0y9zJ3Xy0AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from enum import Enum\n",
    "from typing import Any, NamedTuple, Tuple\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "DEFAULT_MAZE = \"\"\"\n",
    "+-+-+-+-+o+-+-+-+-+-+\n",
    "|   |             | |\n",
    "+ + + +-+-+-+ +-+ + +\n",
    "| | |   |   | | |   |\n",
    "+ +-+-+ +-+ + + + +-+\n",
    "| |   |   | |   |   |\n",
    "+ + + + + + + +-+ +-+\n",
    "|   |   |   | |     |\n",
    "+-+-+-+-+-+-+-+ +-+ +\n",
    "|             |   | |\n",
    "+ +-+-+-+-+ + +-+-+ +\n",
    "|   |       |       |\n",
    "+ + + +-+ +-+ +-+-+-+\n",
    "| | |   |     |     |\n",
    "+ +-+-+ + +-+ + +-+ +\n",
    "| |     | | | |   | |\n",
    "+-+ +-+ + + + +-+ + +\n",
    "|   |   |   |   | | |\n",
    "+ +-+ +-+-+-+-+ + + +\n",
    "|   |       |     | |\n",
    "+-+-+-+-+-+x+-+-+-+-+\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class State(NamedTuple):\n",
    "    x: int\n",
    "    y: int\n",
    "\n",
    "\n",
    "class Action(Enum):\n",
    "    up = 0\n",
    "    down = 1\n",
    "    left = 2\n",
    "    right = 3\n",
    "\n",
    "\n",
    "class Maze:\n",
    "    def __init__(self, maze_str: str = DEFAULT_MAZE):\n",
    "        maze = []\n",
    "        for y, line in enumerate(maze_str.strip().split(\"\\n\")):\n",
    "            line = line.rstrip()\n",
    "            row = []\n",
    "            for x, c in enumerate(line):\n",
    "                if c in {\" \", \"o\", \"x\"}:\n",
    "                    row.append(1)  # spaces are 1s\n",
    "                    if c == \"o\":\n",
    "                        self._start = State(x, y)\n",
    "                    if c == \"x\":\n",
    "                        self._goal = State(x, y)\n",
    "                else:\n",
    "                    row.append(0)  # walls are 0s\n",
    "            maze.append(row)\n",
    "        # self._render_maze = deepcopy(self._maze)\n",
    "        self._maze = maze\n",
    "        self._num_cols = len(maze[0])\n",
    "        self._num_rows = len(maze)\n",
    "        self._ax = None\n",
    "        self._image = None\n",
    "\n",
    "    def get_transition_state_and_cost(self, state: State, action: Action) -> Tuple[State, float]:\n",
    "\n",
    "        if action == Action.left:\n",
    "            next_state = State(state.x - 1, state.y)\n",
    "        if action == Action.right:\n",
    "            next_state = State(state.x + 1, state.y)\n",
    "        if action == Action.up:\n",
    "            next_state = State(state.x, state.y - 1)\n",
    "        if action == Action.down:\n",
    "            next_state = State(state.x, state.y + 1)\n",
    "\n",
    "        # If candidate next state is valid\n",
    "        if (\n",
    "            0 <= next_state.x < self._num_cols\n",
    "            and 0 <= next_state.y < self._num_rows\n",
    "            and self._maze[next_state.y][next_state.x] == 1\n",
    "        ):\n",
    "            return (\n",
    "                next_state,\n",
    "                abs(next_state.x - state.x) + abs(next_state.y - state.y)  # every move costs 1\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                state,\n",
    "                2  # big penalty when hitting a wall\n",
    "            )\n",
    "    \n",
    "    def get_initial_state(self) -> State:\n",
    "        return self._start\n",
    "    \n",
    "    def is_goal(self, state: State) -> bool:\n",
    "        return state == self._goal\n",
    "\n",
    "    def render(self, state: State) -> Any:\n",
    "        if self._ax is None:\n",
    "            fig, ax = plt.subplots(1)\n",
    "            fig.canvas.set_window_title(\"Maze\")\n",
    "            ax.set_aspect(\"equal\")  # set the x and y axes to the same scale\n",
    "            plt.xticks([])  # remove the tick marks by setting to an empty list\n",
    "            plt.yticks([])  # remove the tick marks by setting to an empty list\n",
    "            ax.invert_yaxis()  # invert the y-axis so the first row of data is at the top\n",
    "            self._ax = ax\n",
    "            plt.ion()\n",
    "        maze = deepcopy(self._maze)\n",
    "        maze[self._goal.y][self._goal.x] = 0.7\n",
    "        maze[state.y][state.x] = 0.3\n",
    "        if self._image is None:\n",
    "            self._image = self._ax.imshow(maze)\n",
    "        else:\n",
    "            self._image.set_data(maze)\n",
    "        # self._ax.pcolormesh(maze)\n",
    "        # plt.draw()\n",
    "        plt.pause(0.001)\n",
    "        \n",
    "maze = Maze()\n",
    "maze.render(maze.get_initial_state())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is to bring the blue agent at the top of the maze to the green cell in minimum time (i.e. minimum number of moves).\n",
    "Each move costs 1 but 2 if the agent hits a wall or an obstacle.\n",
    "\n",
    "How would you proceed? In theory, the number of plans is infinite. But even you restrict yourselves to, let's say, plans of at most $N > 0$ steps,\n",
    "then the number of options is bounded by $\\sum_{1 \\leqslant n \\leqslant N} 4^n = \\frac{4^{N+1} - 1}{3}$ since there are 4 possible moves at each step.\n",
    "It is obviously too much for a brute-force search.\n",
    "\n",
    "If you are not convinced, let's consider a very simple 10 by 10 maze without obstacles. We can say that 20 is a reasonable upper bound for the number of steps\n",
    "to go from one corner to the opposite one along the diagonal of the maze (and actually 2 is the right number of steps in that case).\n",
    "The number of plans to explore, if we would do it naively one by one, would be equal to $1.5 \\times 10^{12}$.\n",
    "If each evaluation takes $1 \\; \\text{millisecond}$, it will take $1.5 \\times 10^9 \\; \\text{seconds} = 47.5 \\; \\text{years}$!\n",
    "\n",
    "Obviously, it takes a fraction of seconds for a human to find the solution...\n",
    "So what do we do differently? First, we quickly extract the structure of the problem from our visual inputs, then we use our intuition based on our experience\n",
    "and millions of years of evolution to try to move as much as possible the agent towards the cell goal as if we threw away the obstacles along the path.\n",
    "We then locally adapt this intuitive path in order to avoid the obstacles, and if we are trapped in a dead-end, we then mentally backtrack to the previous position\n",
    "in the maze were we could have taken another intuitive option.\n",
    "\n",
    "Well, there is actually an algorithm which follows the same reasoning: the famous [A* algorithm](https://en.wikipedia.org/wiki/A*_search_algorithm).\n",
    "The algorithmic counterpart of the intuitive guidance is the _heuristic_ function $h : S \\rightarrow R$ which provides a lower bound on the cumulated cost\n",
    "that would be needed to reach a goal state by following an optimal plan.\n",
    "\n",
    "For the maze problem, where all moves cost 1 or 2, the so-called _flying distance_ $h(s) = \\min_{s_g \\in G} \\Vert \\overrightarrow{s_g} - \\overrightarrow{s} \\Vert$\n",
    "is less than the cost of any plan that goes from $s$ to any state in $G$ (here we abuse the notation of a state $s$ as a vector $\\overrightarrow{s}$ defined by\n",
    "the $x$ and $y$ coordinates of the state).\n",
    "Another heuristic which is more informative because closer to the optimal cost is the so-called _Manhattan distance_defined as\n",
    "$h(s) = \\min_{s_g \\in G} \\vert \\overrightarrow{s_g} \\cdot \\overrightarrow{x} - \\overrightarrow{s} \\cdot \\overrightarrow{x} \\vert +\n",
    "                         \\vert \\overrightarrow{s_g} \\cdot \\overrightarrow{y} - \\overrightarrow{s} \\cdot \\overrightarrow{y} \\vert$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A* search algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A* is a graph search algorithm where the planning problem is seen as traversing a [graph](https://en.wikipedia.org/wiki/Graph_(discrete_mathematics))\n",
    "$\\mathcal{G} = (S, A)$ from a root node to some goal nodes.\n",
    "As a user of A* who wants to solve a planning problem, the first thing to do consists in mapping the problem to a graph.\n",
    "\n",
    "As shown in the [pseudo-code of A*](https://en.wikipedia.org/wiki/A*_search_algorithm#Pseudocode), the algorithm iteratively extends the graph by assigning\n",
    "to each new node a score $f(n) = g(n) + h(n)$, where $g(n)$ is the cost of the best current path from the root node to $n$ and $h(n)$ is the heuristic cost\n",
    "of $n$ as defined in the previous section. The $f$-score $f(n)$ represents the best guess of a path leading from the root node to the goal node by going through $n$.\n",
    "\n",
    "The logic of the algorithm is quite simple: it will maintain a list of all the nodes that are not yet expanded, i.e. for which next possibles nodes have not yet been explored,\n",
    "and it iteratively picks the node in the list with the lowest $f$-score to be expanded. Indeed, the nodes with the lowest $f$-scores are more likely to be part\n",
    "of the optimal path to the goal node in the graph. The list of non-expanded explored nodes is called the _open_ list.\n",
    "\n",
    "The algorithm then analyses all the already explored successors of the picked node (if any) and sees if the current best path to them is worth being updated by\n",
    "going instead through the picked node. For this, the algorithm records the best current ancestor of all explored nodes. Then, it _closes_ the picked node.\n",
    "\n",
    "The algorithm finishes when a goal node has been reached _and_ closed, i.e. has been chosen as the best node to close from the open list.\n",
    "A classical mistake consists in exiting the search whenever a goal node has been reached, which is unfortunately *not* sufficient to reach optimality.\n",
    "The optimal path from the root node to the goal node is obtained by reversely following the best ancestor of all nodes from the goal node.\n",
    "\n",
    "A* is proven to return an optimal plan if the heuristic function $h$ is both _admissible_,\n",
    "i.e. $h(n) \\leqslant \\min_{\\pi \\in \\bigcup_{n \\in \\mathbb{N}_+} A^n} C(\\pi) \\; \\text{such that} \\; \\sigma(\\pi)[n-1] \\in G$,\n",
    "and _monotone_ or _consistent_, i.e. $h(n) \\leqslant c(n, n') + h(n')$ for any node $n$ and its successor nodes $n'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background:green\">A* is just one of many graph search algorithms. It is especially worth mentioning that local search algorithms, even if not optimal,\n",
    "can be useful to solve large problems. A large amount of research works focus on automatically generating informative heuristics for complex domains\n",
    "with multidimensional and non-Euclidean state spaces. Those interested can look at this useful [wiki on task planning research](https://planning.wiki/)</span>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b81519d285639899e6ffbf246c769949a19cca29f480f5c915279ae591fdfb44"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 ('seq_dec_mak')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
