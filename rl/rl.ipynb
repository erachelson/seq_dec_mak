{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning course notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment simulation\n",
    "\n",
    "We will start with a simple control environment that you will complete yourselves, then we will move to a more complex open-source aircraft control environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple line control environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin with a very simple control problem consisting in maintaining a moving object along a straight line.\n",
    "We can control its acceleration as shown in the following picture.\n",
    "The objective is to keep minimum the distance between the object and the center line as long as possible.\n",
    "To make the problem a bit interesting, we constrain the norm of the acceleration to be larger than a given value $a_{min}$: $\\forall t > 0, \\Vert a(t) \\Vert_2 \\geqslant a_{min}$.\n",
    "\n",
    "![Line control environment](line_control_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mathematical modeling of the Reinforcement Learning problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to learn to control the object using Reinforcement Learning.\n",
    "The state of the system must contain the minimal information required to update the physics of the system from the action, i.e. the acceleration of the object.\n",
    "In this case, we need at least the position and the speed of the object, thus the state will be defined at any time $t$ by $s(t)=(x(t), y(t), v_x(t), v_y(t))$\n",
    "Moreover, for classical RL algorithms to apply, we need to discretize the time every $\\Delta t$ time units.\n",
    "We can now approximate the physics of the object movement with the following equations, knowing the initial state $s(0)=(x(0), y(0), v_x(0), v_y(0))$:\n",
    "- $\\forall t > 0, v_x(t+\\Delta t) = v_x(t) + a_x(t) \\cdot \\Delta t$ ;\n",
    "- $\\forall t > 0, x(t+\\Delta t) = x(t) + v_x(t) \\cdot \\Delta t$ ;\n",
    "- $\\forall t > 0, v_y(t+\\Delta t) = v_y(t) + a_y(t) \\cdot \\Delta t$ ;\n",
    "- $\\forall t > 0, y(t+\\Delta t) = y(t) + v_y(t) \\cdot \\Delta t$ ;\n",
    "\n",
    "Since we want to keep minimum the distance between the center line and the object, we will model the reward signal at any time $t$ by $r(t) = e^{-\\vert y(t) \\vert}$.\n",
    "By doing so, an RL agent who will try to maximize the cumulated sum of (discounted) rewards will try to keep $y(t)$ as close as possible to $0$ at any time step.\n",
    "There are two possible ways to enforce the constraint $\\Vert a(t) \\Vert_2 \\geqslant a_{min}$ in RL:\n",
    "- either by ensuring that the algorithm will only select such actions ;\n",
    "- or by associating a very large penalty (i.e. negative reward) to transitions labelled with such actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation in a Gym environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[OpenAI Gym](https://gym.openai.com/) is a popular Python software library to model RL environments in a standard way which can be exploited RL algorithm libraries like [RLlib](https://www.ray.io/rllib) or [Stable Baselines](https://github.com/DLR-RM/stable-baselines3).\n",
    "OpenAI Gym - or Gym in short - provides well-known environment implementations like CartPole, but we can also implement our own environment by following their standards, which will allow us to solve our environment using well-implemented and efficient RL algorithms from the aforementioned libraries.\n",
    "All we have to do is to implement a domain class with the following methods:\n",
    "```python\n",
    "class MyEnvironement:\n",
    "    def __init__(self):\n",
    "        # Declare your variables here, including the environment's state.\n",
    "        # Declare also the action and observation (i.e. state) spaces: the action space is used by the algorithm\n",
    "        # to select actions while the observation space is used by Deep RL algorithms to properly initialize\n",
    "        # the observation (i.e. state) layer of the tensors.\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        # Initialize and return the initial state of the environment\n",
    "        pass\n",
    "\n",
    "    def step(self, action):\n",
    "        # Perform one simulation step of the environment, i.e. compute the state resulting from applying the given action in the current state.\n",
    "        # Don't forget to update the environment's state so that the next call to the step method will reason about the updated state.\n",
    "        # Must return a tuple (state, reward, done, info) where done is true if the episode should stop now and info is a dictionary that can be left empty.\n",
    "        pass\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        # If you want to render something at each simulation step (e.g. an image, some text, etc.)\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It's your turn!\n",
    "Please fill in the missing lines in the definition below of the Gym environment which implements \"simple line control\" problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym.envs.classic_control import rendering\n",
    "from math import sqrt, exp, fabs\n",
    "\n",
    "\n",
    "HORIZON = 500\n",
    "ACCELERATION_MIN = 0.5\n",
    "PENALTY = -1000.\n",
    "\n",
    "\n",
    "class SimpleLineControlGymEnv:\n",
    "    \"\"\"This class mimics an OpenAI Gym environment\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize GymDomain.\n",
    "        # Parameters\n",
    "        gym_env: The Gym environment (gym.env) to wrap.\n",
    "        \"\"\"\n",
    "        inf = np.finfo(np.float32).max\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            np.array([-1.0, -1.0]), np.array([1.0, 1.0]), dtype=np.float32\n",
    "        )\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            np.array([-inf, -inf, -inf, -inf]),\n",
    "            np.array([inf, inf, inf, inf]),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        self._delta_t = 0.001\n",
    "        self._init_pos_x = 0.0\n",
    "        self._init_pos_y = 0.5\n",
    "        self._init_speed_x = 10.0\n",
    "        self._init_speed_y = 1.0\n",
    "        self._pos_x = None\n",
    "        self._pos_y = None\n",
    "        self._speed_x = None\n",
    "        self._speed_y = None\n",
    "        self.viewer = None\n",
    "        self._path = []\n",
    "\n",
    "    def get_state(self):\n",
    "        return np.array(\n",
    "            [self._pos_x, self._pos_y, self._speed_x, self._speed_y], dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self._pos_x = state[0]\n",
    "        self._pos_y = state[1]\n",
    "        self._speed_x = state[2]\n",
    "        self._speed_y = state[3]\n",
    "\n",
    "    def reset(self):\n",
    "        self._pos_x = self._init_pos_x\n",
    "        self._pos_y = self._init_pos_y\n",
    "        self._speed_x = self._init_speed_x\n",
    "        self._speed_y = self._init_speed_y\n",
    "        self._path = []\n",
    "        return np.array(\n",
    "            [self._pos_x, self._pos_y, self._speed_x, self._speed_y], dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        if sqrt(action[0]*action[0] + action[1]*action[1]) < ACCELERATION_MIN:\n",
    "            obs = np.array(\n",
    "                [self._pos_x, self._pos_y, self._speed_x, self._speed_y], dtype=np.float32\n",
    "            )\n",
    "            return obs, PENALTY, True, {}\n",
    "        self._speed_x = self._speed_x + action[0] * self._delta_t\n",
    "        self._speed_y = self._speed_y + action[1] * self._delta_t\n",
    "        self._pos_x = self._pos_x + self._delta_t * self._speed_x\n",
    "        self._pos_y = self._pos_y + self._delta_t * self._speed_y\n",
    "        obs = np.array(\n",
    "            [self._pos_x, self._pos_y, self._speed_x, self._speed_y], dtype=np.float32\n",
    "        )\n",
    "        reward = exp(-fabs(self._pos_y))\n",
    "        done = bool(fabs(self._pos_y) > 1.0)\n",
    "        self._path.append((self._pos_x, self._pos_y))\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        screen_width = 600\n",
    "        screen_height = 400\n",
    "\n",
    "        if self.viewer is None:\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            self.track = rendering.Line(\n",
    "                (0, screen_height / 2), (screen_width, screen_height / 2)\n",
    "            )\n",
    "            self.track.set_color(0, 0, 1)\n",
    "            self.viewer.add_geom(self.track)\n",
    "            self.traj = rendering.PolyLine([], False)\n",
    "            self.traj.set_color(1, 0, 0)\n",
    "            self.traj.set_linewidth(3)\n",
    "            self.viewer.add_geom(self.traj)\n",
    "\n",
    "        if len(self.traj.v) != len(self._path):\n",
    "            self.traj.v = []\n",
    "            for p in self._path:\n",
    "                self.traj.v.append((p[0] * 100, screen_height / 2 + p[1] * 100))\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aircraft taxiing control environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we investigate a more complex control problem consisting in controlling a flying aircraft.\n",
    "Based on the Gym environments from the [gym-jsbsim](https://github.com/galleon/gym-jsbsim) library which simulate aircraft physics,\n",
    "we will try to learn to follow a certain heading and altitude so that every 150 sec a new target heading and altitude are set.\n",
    "The environment is explained [here](https://github.com/galleon/gym-jsbsim/blob/master/README.md#heading-and-altitude-task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_jsbsim\n",
    "\n",
    "env = gym.make(\"GymJsbsim-HeadingAltitudeControlTask-v0\")\n",
    "env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "   action = env.action_space.sample()\n",
    "   state, reward, done, _ = env.step(action)\n",
    "   print('state: {}'.format(state))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b81519d285639899e6ffbf246c769949a19cca29f480f5c915279ae591fdfb44"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 ('seq_dec_mak')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
